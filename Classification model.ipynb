{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1430, 5)\n",
      "Index(['Unnamed: 0', 'tweet', 'label', 'categories', 'cleaned_tweet'], dtype='object')\n",
      "1430\n"
     ]
    }
   ],
   "source": [
    "import preprocessor\n",
    "import demoji\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def preprocess_text(text):\n",
    "    # Use tweet-preprocessor to clean tweets\n",
    "    cleaned_text = preprocessor.clean(text)\n",
    "    # Remove emojis\n",
    "    cleaned_text = remove_emojis(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "# Load your dataset with columns 'tweet' and 'categories'\n",
    "df = pd.read_csv(\"/Users/Hsuweic/Desktop/AI4healthcare/dataset/dataset_categories.csv\")\n",
    "hate_speech = df[df['label'] == 1].copy() \n",
    "# Reset the index to ensure it is consecutive\n",
    "hate_speech.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preprocess text\n",
    "hate_speech['cleaned_tweet'] = hate_speech['tweet'].apply(preprocess_text)\n",
    "\n",
    "print(hate_speech.shape)\n",
    "print(hate_speech.columns)\n",
    "print(len(hate_speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Dictionary:\n",
      "{'Race': 0, 'Sexual Orientation': 1, 'Gender': 2, 'Religion': 3, 'Disability': 4, 'Physical Appearance': 5, 'Class': 6, 'Ethnicity': 7, 'Behavior': 8}\n",
      "   Unnamed: 0                                              tweet  label   \n",
      "0           1  Scandal: A Negro Bed Wench's Fairy-tale and pr...      0  \\\n",
      "1           3  Why does this faggot in front of me have to ge...      1   \n",
      "2           4  RT @ERlKTweets: what the fuck is this disgusti...      0   \n",
      "3           6  @PaulMoon8 @yurista4life @justinmiculka01 you ...      2   \n",
      "4          15  Black cops want to kill white citizens. Spic c...      0   \n",
      "5          16  RT \"@TIMAtheRACER: Wow do we have another fagg...      1   \n",
      "6          17  @kitlange stop oppressing me! Have you heard o...      3   \n",
      "7          18  I hate when white trash try to act like they'r...      0   \n",
      "8          19  @klejdys He may be 'hustlin' but he studied Cr...      0   \n",
      "9          21  &#8220;@FreddieGibbs: Tea bag a bitch.&#8221; ...      2   \n",
      "\n",
      "           categories                                      cleaned_tweet  \n",
      "0                Race  Scandal: A Negro Bed Wench's Fairy-tale and pr...  \n",
      "1  Sexual Orientation  Why does this faggot in front of me have to ge...  \n",
      "2                Race  : what the fuck is this disgusting nigger from...  \n",
      "3              Gender  you fuckin Yankees I'll be there in months I'l...  \n",
      "4                Race  Black cops want to kill white citizens. Spic c...  \n",
      "5  Sexual Orientation  \": Wow do we have another faggot? &;: TIMAtheR...  \n",
      "6            Religion  stop oppressing me! Have you heard of free spe...  \n",
      "7                Race  I hate when white trash try to act like they'r...  \n",
      "8                Race  He may be 'hustlin' but he studied Critical (R...  \n",
      "9              Gender                    &;: Tea bag a bitch.&; Pahahaha  \n"
     ]
    }
   ],
   "source": [
    "# Convert categories to numerical labels\n",
    "label_dict = {category: idx for idx, category in enumerate(hate_speech['categories'].unique())}\n",
    "hate_speech['label'] = hate_speech['categories'].map(label_dict)\n",
    "\n",
    "# Print label_dict to see the mapping\n",
    "print(\"Label Dictionary:\")\n",
    "print(label_dict)\n",
    "print(hate_speech.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    : A real nigga gone teach his bitch the game n...\n",
      "1    Sick of your coon antics RT : Accuracy RT : &;...\n",
      "2                              This niggah know wassup\n",
      "3    : if u dont want your heart broken then dont c...\n",
      "4    sweet comeback u fuckin herb I dnt watch socce...\n",
      "5    : Do you ever just see someone's account and t...\n",
      "6    they are wrong you are a nigger always causung...\n",
      "7    fucking block your white ass because you try t...\n",
      "8    : if i dont get drafted for the squad we gonna...\n",
      "9    \"We're out here, and we're queer!\"\" , , , hut!...\n",
      "Name: cleaned_tweet, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "1144\n",
      "(1144,)\n",
      "cleaned_tweet\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    hate_speech['cleaned_tweet'],  # Features\n",
    "    hate_speech['label'],  # Labels\n",
    "    test_size=0.2,  # Adjust as needed\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Reset the index to ensure it is consecutive\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_train.head(10))\n",
    "print(type(X_train))\n",
    "print(len(X_train))\n",
    "print(X_train.shape)\n",
    "print(X_train.name)\n",
    "# print(y_train.head(10))\n",
    "# print(len(y_train))\n",
    "# print(X_test.head(10))\n",
    "# print(len(X_test))\n",
    "# print(y_test.head(10))\n",
    "# print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression with Multinomial (Softmax):\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "               Race       0.67      0.92      0.77       109\n",
      " Sexual Orientation       0.91      0.90      0.90        78\n",
      "             Gender       0.57      0.64      0.60        50\n",
      "           Religion       0.00      0.00      0.00        13\n",
      "         Disability       0.67      0.25      0.36         8\n",
      "Physical Appearance       0.00      0.00      0.00        16\n",
      "              Class       0.00      0.00      0.00         3\n",
      "          Ethnicity       0.00      0.00      0.00         6\n",
      "           Behavior       0.00      0.00      0.00         3\n",
      "\n",
      "           accuracy                           0.71       286\n",
      "          macro avg       0.31      0.30      0.29       286\n",
      "       weighted avg       0.62      0.71      0.66       286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression with multinomial (softmax)\n",
    "logreg_multinomial = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
    "logreg_multinomial.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_multinomial = logreg_multinomial.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model with multinomial (softmax)\n",
    "print(\"\\nLogistic Regression with Multinomial (Softmax):\")\n",
    "print(classification_report(y_test, y_pred_multinomial, target_names=label_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTweet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load BERTweet model and tokenizer\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "hate_categories = 9  # Number of categories\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=hate_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1430\n",
      "0       [input_ids, token_type_ids, attention_mask]\n",
      "1       [input_ids, token_type_ids, attention_mask]\n",
      "2       [input_ids, token_type_ids, attention_mask]\n",
      "3       [input_ids, token_type_ids, attention_mask]\n",
      "4       [input_ids, token_type_ids, attention_mask]\n",
      "                           ...                     \n",
      "1425    [input_ids, token_type_ids, attention_mask]\n",
      "1426    [input_ids, token_type_ids, attention_mask]\n",
      "1427    [input_ids, token_type_ids, attention_mask]\n",
      "1428    [input_ids, token_type_ids, attention_mask]\n",
      "1429    [input_ids, token_type_ids, attention_mask]\n",
      "Name: cleaned_tweet, Length: 1430, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input data\n",
    "max_length = 128  # You can adjust this based on your requirements\n",
    "\n",
    "def tokenize_data(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize input data\n",
    "tokenized_data = hate_speech['cleaned_tweet'].apply(tokenize_data)\n",
    "\n",
    "# # Convert categories to numerical labels\n",
    "# label_dict = {category: idx for idx, category in enumerate(hate_speech['categories'].unique())}\n",
    "# hate_speech['label'] = hate_speech['categories'].map(label_dict)\n",
    "\n",
    "# # Print label_dict to see the mapping\n",
    "# print(\"Label Dictionary:\")\n",
    "# print(label_dict)\n",
    "# # print(hate_speech['label'].unique())\n",
    "print(len(tokenized_data))\n",
    "print(tokenized_data)\n",
    "\n",
    "# Prepare input tensors\n",
    "input_ids = torch.cat([tokenized_data[i]['input_ids'] for i in range(len(tokenized_data))], dim=0)\n",
    "attention_masks = torch.cat([tokenized_data[i]['attention_mask'] for i in range(len(tokenized_data))], dim=0)\n",
    "labels = torch.tensor(hate_speech['label'].values)\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144\n",
      "0       [input_ids, token_type_ids, attention_mask]\n",
      "1       [input_ids, token_type_ids, attention_mask]\n",
      "2       [input_ids, token_type_ids, attention_mask]\n",
      "3       [input_ids, token_type_ids, attention_mask]\n",
      "4       [input_ids, token_type_ids, attention_mask]\n",
      "                           ...                     \n",
      "1139    [input_ids, token_type_ids, attention_mask]\n",
      "1140    [input_ids, token_type_ids, attention_mask]\n",
      "1141    [input_ids, token_type_ids, attention_mask]\n",
      "1142    [input_ids, token_type_ids, attention_mask]\n",
      "1143    [input_ids, token_type_ids, attention_mask]\n",
      "Name: cleaned_tweet, Length: 1144, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize and prepare input data for training set\n",
    "tokenized_train = X_train.apply(tokenize_data)\n",
    "print(len(tokenized_train))\n",
    "print(tokenized_train)\n",
    "\n",
    "input_ids_train = torch.cat([tokenized_train[i]['input_ids'] for i in range(len(tokenized_train))], dim=0)\n",
    "attention_masks_train = torch.cat([tokenized_train[i]['attention_mask'] for i in range(len(tokenized_train))], dim=0)\n",
    "labels_train = torch.tensor(y_train.values)\n",
    "\n",
    "# Create DataLoader for training set\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "=== Start Batch 0 ===\n",
      "1.8367869853973389\n",
      "=== Start Batch 1 ===\n",
      "3.1182576417922974\n",
      "=== Start Batch 2 ===\n",
      "4.283602952957153\n",
      "=== Start Batch 3 ===\n",
      "5.771800637245178\n",
      "=== Start Batch 4 ===\n",
      "7.332326054573059\n",
      "=== Start Batch 5 ===\n",
      "8.454647541046143\n",
      "=== Start Batch 6 ===\n",
      "9.653226494789124\n",
      "=== Start Batch 7 ===\n",
      "11.152984857559204\n",
      "=== Start Batch 8 ===\n",
      "12.311897277832031\n",
      "=== Start Batch 9 ===\n",
      "14.313760042190552\n",
      "=== Start Batch 10 ===\n",
      "15.786075830459595\n",
      "=== Start Batch 11 ===\n",
      "17.31002676486969\n",
      "=== Start Batch 12 ===\n",
      "18.63794243335724\n",
      "=== Start Batch 13 ===\n",
      "20.45743465423584\n",
      "=== Start Batch 14 ===\n",
      "22.0542995929718\n",
      "=== Start Batch 15 ===\n",
      "23.970901489257812\n",
      "=== Start Batch 16 ===\n",
      "26.334065198898315\n",
      "=== Start Batch 17 ===\n",
      "28.214422583580017\n",
      "=== Start Batch 18 ===\n",
      "29.795998454093933\n",
      "=== Start Batch 19 ===\n",
      "31.078032851219177\n",
      "=== Start Batch 20 ===\n",
      "32.644795060157776\n",
      "=== Start Batch 21 ===\n",
      "33.91679561138153\n",
      "=== Start Batch 22 ===\n",
      "35.66980850696564\n",
      "=== Start Batch 23 ===\n",
      "37.145241498947144\n",
      "=== Start Batch 24 ===\n",
      "38.71804320812225\n",
      "=== Start Batch 25 ===\n",
      "40.11951768398285\n",
      "=== Start Batch 26 ===\n",
      "41.71117079257965\n",
      "=== Start Batch 27 ===\n",
      "43.08411228656769\n",
      "=== Start Batch 28 ===\n",
      "44.725435853004456\n",
      "=== Start Batch 29 ===\n",
      "46.00631630420685\n",
      "=== Start Batch 30 ===\n",
      "47.41183686256409\n",
      "=== Start Batch 31 ===\n",
      "49.6155059337616\n",
      "=== Start Batch 32 ===\n",
      "51.41013312339783\n",
      "=== Start Batch 33 ===\n",
      "53.139551639556885\n",
      "=== Start Batch 34 ===\n",
      "54.759928464889526\n",
      "=== Start Batch 35 ===\n",
      "56.05710220336914\n",
      "=== Start Batch 36 ===\n",
      "57.22048008441925\n",
      "=== Start Batch 37 ===\n",
      "59.64606201648712\n",
      "=== Start Batch 38 ===\n",
      "60.83259177207947\n",
      "=== Start Batch 39 ===\n",
      "62.6183397769928\n",
      "=== Start Batch 40 ===\n",
      "64.22070622444153\n",
      "=== Start Batch 41 ===\n",
      "65.8484799861908\n",
      "=== Start Batch 42 ===\n",
      "67.77797842025757\n",
      "=== Start Batch 43 ===\n",
      "69.73702991008759\n",
      "=== Start Batch 44 ===\n",
      "71.43656849861145\n",
      "=== Start Batch 45 ===\n",
      "72.9691276550293\n",
      "=== Start Batch 46 ===\n",
      "74.84415173530579\n",
      "=== Start Batch 47 ===\n",
      "76.37100124359131\n",
      "=== Start Batch 48 ===\n",
      "77.70482337474823\n",
      "=== Start Batch 49 ===\n",
      "79.01250302791595\n",
      "=== Start Batch 50 ===\n",
      "81.02096807956696\n",
      "=== Start Batch 51 ===\n",
      "82.85040366649628\n",
      "=== Start Batch 52 ===\n",
      "84.1473228931427\n",
      "=== Start Batch 53 ===\n",
      "85.95014870166779\n",
      "=== Start Batch 54 ===\n",
      "87.17850971221924\n",
      "=== Start Batch 55 ===\n",
      "88.35727870464325\n",
      "=== Start Batch 56 ===\n",
      "89.5442601442337\n",
      "=== Start Batch 57 ===\n",
      "91.41848230361938\n",
      "=== Start Batch 58 ===\n",
      "92.67150950431824\n",
      "=== Start Batch 59 ===\n",
      "93.99358522891998\n",
      "=== Start Batch 60 ===\n",
      "95.80133306980133\n",
      "=== Start Batch 61 ===\n",
      "96.96905791759491\n",
      "=== Start Batch 62 ===\n",
      "97.88505148887634\n",
      "=== Start Batch 63 ===\n",
      "99.90257096290588\n",
      "=== Start Batch 64 ===\n",
      "102.04175901412964\n",
      "=== Start Batch 65 ===\n",
      "104.60278630256653\n",
      "=== Start Batch 66 ===\n",
      "105.88536977767944\n",
      "=== Start Batch 67 ===\n",
      "107.17393684387207\n",
      "=== Start Batch 68 ===\n",
      "108.76754581928253\n",
      "=== Start Batch 69 ===\n",
      "110.53255140781403\n",
      "=== Start Batch 70 ===\n",
      "111.44622349739075\n",
      "=== Start Batch 71 ===\n",
      "113.06016266345978\n",
      "=== Start Batch 72 ===\n",
      "114.20024633407593\n",
      "=== Start Batch 73 ===\n",
      "115.42820739746094\n",
      "=== Start Batch 74 ===\n",
      "116.67503476142883\n",
      "=== Start Batch 75 ===\n",
      "118.04958951473236\n",
      "=== Start Batch 76 ===\n",
      "119.48512697219849\n",
      "=== Start Batch 77 ===\n",
      "120.58161211013794\n",
      "=== Start Batch 78 ===\n",
      "121.580983877182\n",
      "=== Start Batch 79 ===\n",
      "122.78993439674377\n",
      "=== Start Batch 80 ===\n",
      "124.43893253803253\n",
      "=== Start Batch 81 ===\n",
      "125.40706419944763\n",
      "=== Start Batch 82 ===\n",
      "126.5299346446991\n",
      "=== Start Batch 83 ===\n",
      "127.67827451229095\n",
      "=== Start Batch 84 ===\n",
      "129.01296818256378\n",
      "=== Start Batch 85 ===\n",
      "130.361172914505\n",
      "=== Start Batch 86 ===\n",
      "131.78421545028687\n",
      "=== Start Batch 87 ===\n",
      "132.91328394412994\n",
      "=== Start Batch 88 ===\n",
      "134.36254036426544\n",
      "=== Start Batch 89 ===\n",
      "135.7319291830063\n",
      "=== Start Batch 90 ===\n",
      "137.24788522720337\n",
      "=== Start Batch 91 ===\n",
      "138.42018485069275\n",
      "=== Start Batch 92 ===\n",
      "140.18578553199768\n",
      "=== Start Batch 93 ===\n",
      "141.4368189573288\n",
      "=== Start Batch 94 ===\n",
      "142.68280851840973\n",
      "=== Start Batch 95 ===\n",
      "144.23411059379578\n",
      "=== Start Batch 96 ===\n",
      "146.44119668006897\n",
      "=== Start Batch 97 ===\n",
      "147.5065323114395\n",
      "=== Start Batch 98 ===\n",
      "149.528923869133\n",
      "=== Start Batch 99 ===\n",
      "150.87222754955292\n",
      "=== Start Batch 100 ===\n",
      "152.4248914718628\n",
      "=== Start Batch 101 ===\n",
      "153.42749333381653\n",
      "=== Start Batch 102 ===\n",
      "155.1088466644287\n",
      "=== Start Batch 103 ===\n",
      "157.5072729587555\n",
      "=== Start Batch 104 ===\n",
      "158.56029045581818\n",
      "=== Start Batch 105 ===\n",
      "159.82319808006287\n",
      "=== Start Batch 106 ===\n",
      "161.41610491275787\n",
      "=== Start Batch 107 ===\n",
      "162.66722321510315\n",
      "=== Start Batch 108 ===\n",
      "164.26091468334198\n",
      "=== Start Batch 109 ===\n",
      "165.45831906795502\n",
      "=== Start Batch 110 ===\n",
      "167.2664407491684\n",
      "=== Start Batch 111 ===\n",
      "168.69330668449402\n",
      "=== Start Batch 112 ===\n",
      "170.00243020057678\n",
      "=== Start Batch 113 ===\n",
      "171.6035656929016\n",
      "=== Start Batch 114 ===\n",
      "172.65598368644714\n",
      "=== Start Batch 115 ===\n",
      "174.35132730007172\n",
      "=== Start Batch 116 ===\n",
      "175.56195414066315\n",
      "=== Start Batch 117 ===\n",
      "176.69095742702484\n",
      "=== Start Batch 118 ===\n",
      "178.69594013690948\n",
      "=== Start Batch 119 ===\n",
      "179.91652309894562\n",
      "=== Start Batch 120 ===\n",
      "181.43834745883942\n",
      "=== Start Batch 121 ===\n",
      "183.1061509847641\n",
      "=== Start Batch 122 ===\n",
      "184.74180674552917\n",
      "=== Start Batch 123 ===\n",
      "186.23295402526855\n",
      "=== Start Batch 124 ===\n",
      "187.6279355287552\n",
      "=== Start Batch 125 ===\n",
      "188.7332385778427\n",
      "=== Start Batch 126 ===\n",
      "190.6227170228958\n",
      "=== Start Batch 127 ===\n",
      "192.36736381053925\n",
      "=== Start Batch 128 ===\n",
      "193.84541511535645\n",
      "=== Start Batch 129 ===\n",
      "195.85937523841858\n",
      "=== Start Batch 130 ===\n",
      "197.3833395242691\n",
      "=== Start Batch 131 ===\n",
      "199.06606304645538\n",
      "=== Start Batch 132 ===\n",
      "200.2498232126236\n",
      "=== Start Batch 133 ===\n",
      "201.73350524902344\n",
      "=== Start Batch 134 ===\n",
      "203.28836393356323\n",
      "=== Start Batch 135 ===\n",
      "205.108851313591\n",
      "=== Start Batch 136 ===\n",
      "206.49475944042206\n",
      "=== Start Batch 137 ===\n",
      "207.85199999809265\n",
      "=== Start Batch 138 ===\n",
      "209.8735706806183\n",
      "=== Start Batch 139 ===\n",
      "211.7779824733734\n",
      "=== Start Batch 140 ===\n",
      "213.53900456428528\n",
      "=== Start Batch 141 ===\n",
      "214.62642776966095\n",
      "=== Start Batch 142 ===\n",
      "215.89370548725128\n",
      "Epoch 1/5, Loss: 1.5097461922185405\n",
      "Start Training\n",
      "=== Start Batch 0 ===\n",
      "1.603952407836914\n",
      "=== Start Batch 1 ===\n",
      "3.759681463241577\n",
      "=== Start Batch 2 ===\n",
      "5.471355319023132\n",
      "=== Start Batch 3 ===\n",
      "6.856605529785156\n",
      "=== Start Batch 4 ===\n",
      "8.500091552734375\n",
      "=== Start Batch 5 ===\n",
      "9.66225814819336\n",
      "=== Start Batch 6 ===\n",
      "10.639987468719482\n",
      "=== Start Batch 7 ===\n",
      "11.809458494186401\n",
      "=== Start Batch 8 ===\n",
      "13.053290486335754\n",
      "=== Start Batch 9 ===\n",
      "14.109887838363647\n",
      "=== Start Batch 10 ===\n",
      "15.095445036888123\n",
      "=== Start Batch 11 ===\n",
      "16.96879482269287\n",
      "=== Start Batch 12 ===\n",
      "17.99889039993286\n",
      "=== Start Batch 13 ===\n",
      "19.929609894752502\n",
      "=== Start Batch 14 ===\n",
      "21.637951612472534\n",
      "=== Start Batch 15 ===\n",
      "23.00040102005005\n",
      "=== Start Batch 16 ===\n",
      "24.444175720214844\n",
      "=== Start Batch 17 ===\n",
      "25.908705353736877\n",
      "=== Start Batch 18 ===\n",
      "27.06276047229767\n",
      "=== Start Batch 19 ===\n",
      "28.415393948554993\n",
      "=== Start Batch 20 ===\n",
      "30.30039894580841\n",
      "=== Start Batch 21 ===\n",
      "31.310415625572205\n",
      "=== Start Batch 22 ===\n",
      "32.56145524978638\n",
      "=== Start Batch 23 ===\n",
      "34.19690704345703\n",
      "=== Start Batch 24 ===\n",
      "36.1388224363327\n",
      "=== Start Batch 25 ===\n",
      "37.77407515048981\n",
      "=== Start Batch 26 ===\n",
      "39.10471260547638\n",
      "=== Start Batch 27 ===\n",
      "40.51177966594696\n",
      "=== Start Batch 28 ===\n",
      "41.91278946399689\n",
      "=== Start Batch 29 ===\n",
      "43.00260782241821\n",
      "=== Start Batch 30 ===\n",
      "44.442655086517334\n",
      "=== Start Batch 31 ===\n",
      "45.67894744873047\n",
      "=== Start Batch 32 ===\n",
      "47.33125972747803\n",
      "=== Start Batch 33 ===\n",
      "48.538039326667786\n",
      "=== Start Batch 34 ===\n",
      "50.08528113365173\n",
      "=== Start Batch 35 ===\n",
      "51.82431948184967\n",
      "=== Start Batch 36 ===\n",
      "54.123096346855164\n",
      "=== Start Batch 37 ===\n",
      "54.994602501392365\n",
      "=== Start Batch 38 ===\n",
      "56.181796848773956\n",
      "=== Start Batch 39 ===\n",
      "57.31325739622116\n",
      "=== Start Batch 40 ===\n",
      "58.85548883676529\n",
      "=== Start Batch 41 ===\n",
      "59.828772485256195\n",
      "=== Start Batch 42 ===\n",
      "61.06472820043564\n",
      "=== Start Batch 43 ===\n",
      "62.32524412870407\n",
      "=== Start Batch 44 ===\n",
      "63.363119423389435\n",
      "=== Start Batch 45 ===\n",
      "64.39287823438644\n",
      "=== Start Batch 46 ===\n",
      "65.55605465173721\n",
      "=== Start Batch 47 ===\n",
      "66.82927161455154\n",
      "=== Start Batch 48 ===\n",
      "67.8966127038002\n",
      "=== Start Batch 49 ===\n",
      "68.94337469339371\n",
      "=== Start Batch 50 ===\n",
      "70.32093411684036\n",
      "=== Start Batch 51 ===\n",
      "71.53166776895523\n",
      "=== Start Batch 52 ===\n",
      "72.73069375753403\n",
      "=== Start Batch 53 ===\n",
      "73.73875600099564\n",
      "=== Start Batch 54 ===\n",
      "75.22007924318314\n",
      "=== Start Batch 55 ===\n",
      "76.40541213750839\n",
      "=== Start Batch 56 ===\n",
      "77.68989735841751\n",
      "=== Start Batch 57 ===\n",
      "79.51397544145584\n",
      "=== Start Batch 58 ===\n",
      "80.37101048231125\n",
      "=== Start Batch 59 ===\n",
      "82.05603045225143\n",
      "=== Start Batch 60 ===\n",
      "82.82398790121078\n",
      "=== Start Batch 61 ===\n",
      "83.81802868843079\n",
      "=== Start Batch 62 ===\n",
      "85.69326722621918\n",
      "=== Start Batch 63 ===\n",
      "86.68393737077713\n",
      "=== Start Batch 64 ===\n",
      "88.06702989339828\n",
      "=== Start Batch 65 ===\n",
      "88.97707331180573\n",
      "=== Start Batch 66 ===\n",
      "90.47736060619354\n",
      "=== Start Batch 67 ===\n",
      "92.14388704299927\n",
      "=== Start Batch 68 ===\n",
      "93.17737305164337\n",
      "=== Start Batch 69 ===\n",
      "94.26652467250824\n",
      "=== Start Batch 70 ===\n",
      "95.59288096427917\n",
      "=== Start Batch 71 ===\n",
      "97.27586543560028\n",
      "=== Start Batch 72 ===\n",
      "97.91324299573898\n",
      "=== Start Batch 73 ===\n",
      "98.60737401247025\n",
      "=== Start Batch 74 ===\n",
      "100.09484225511551\n",
      "=== Start Batch 75 ===\n",
      "101.55896145105362\n",
      "=== Start Batch 76 ===\n",
      "102.43379807472229\n",
      "=== Start Batch 77 ===\n",
      "103.67396605014801\n",
      "=== Start Batch 78 ===\n",
      "104.98251140117645\n",
      "=== Start Batch 79 ===\n",
      "105.63068675994873\n",
      "=== Start Batch 80 ===\n",
      "106.46854251623154\n",
      "=== Start Batch 81 ===\n",
      "107.33584451675415\n",
      "=== Start Batch 82 ===\n",
      "107.98988366127014\n",
      "=== Start Batch 83 ===\n",
      "109.51274597644806\n",
      "=== Start Batch 84 ===\n",
      "110.8640855550766\n",
      "=== Start Batch 85 ===\n",
      "112.0312477350235\n",
      "=== Start Batch 86 ===\n",
      "112.77521347999573\n",
      "=== Start Batch 87 ===\n",
      "113.52540647983551\n",
      "=== Start Batch 88 ===\n",
      "114.01441857218742\n",
      "=== Start Batch 89 ===\n",
      "115.19380757212639\n",
      "=== Start Batch 90 ===\n",
      "115.98300829529762\n",
      "=== Start Batch 91 ===\n",
      "116.92909267544746\n",
      "=== Start Batch 92 ===\n",
      "118.45629301667213\n",
      "=== Start Batch 93 ===\n",
      "119.49640110135078\n",
      "=== Start Batch 94 ===\n",
      "120.25150409340858\n",
      "=== Start Batch 95 ===\n",
      "120.97364839911461\n",
      "=== Start Batch 96 ===\n",
      "121.94703343510628\n",
      "=== Start Batch 97 ===\n",
      "123.52294996380806\n",
      "=== Start Batch 98 ===\n",
      "125.2388416826725\n",
      "=== Start Batch 99 ===\n",
      "125.79125615954399\n",
      "=== Start Batch 100 ===\n",
      "126.59134861826897\n",
      "=== Start Batch 101 ===\n",
      "126.90542906522751\n",
      "=== Start Batch 102 ===\n",
      "128.43004363775253\n",
      "=== Start Batch 103 ===\n",
      "129.1117667555809\n",
      "=== Start Batch 104 ===\n",
      "130.37452787160873\n",
      "=== Start Batch 105 ===\n",
      "132.01312285661697\n",
      "=== Start Batch 106 ===\n",
      "133.4200478196144\n",
      "=== Start Batch 107 ===\n",
      "134.0999670624733\n",
      "=== Start Batch 108 ===\n",
      "135.32636612653732\n",
      "=== Start Batch 109 ===\n",
      "136.351746737957\n",
      "=== Start Batch 110 ===\n",
      "137.02130728960037\n",
      "=== Start Batch 111 ===\n",
      "138.26249331235886\n",
      "=== Start Batch 112 ===\n",
      "139.18384087085724\n",
      "=== Start Batch 113 ===\n",
      "139.89476931095123\n",
      "=== Start Batch 114 ===\n",
      "140.84607034921646\n",
      "=== Start Batch 115 ===\n",
      "142.04452151060104\n",
      "=== Start Batch 116 ===\n",
      "143.00786298513412\n",
      "=== Start Batch 117 ===\n",
      "144.22501116991043\n",
      "=== Start Batch 118 ===\n",
      "145.68209213018417\n",
      "=== Start Batch 119 ===\n",
      "146.5593785047531\n",
      "=== Start Batch 120 ===\n",
      "147.58887445926666\n",
      "=== Start Batch 121 ===\n",
      "148.21752899885178\n",
      "=== Start Batch 122 ===\n",
      "149.59548729658127\n",
      "=== Start Batch 123 ===\n",
      "151.0118686556816\n",
      "=== Start Batch 124 ===\n",
      "152.1047949194908\n",
      "=== Start Batch 125 ===\n",
      "152.6350655555725\n",
      "=== Start Batch 126 ===\n",
      "153.52340787649155\n",
      "=== Start Batch 127 ===\n",
      "154.06912064552307\n",
      "=== Start Batch 128 ===\n",
      "154.39788776636124\n",
      "=== Start Batch 129 ===\n",
      "155.4005051255226\n",
      "=== Start Batch 130 ===\n",
      "156.03945022821426\n",
      "=== Start Batch 131 ===\n",
      "156.78627985715866\n",
      "=== Start Batch 132 ===\n",
      "157.39446383714676\n",
      "=== Start Batch 133 ===\n",
      "158.06157302856445\n",
      "=== Start Batch 134 ===\n",
      "159.5543978214264\n",
      "=== Start Batch 135 ===\n",
      "160.55920279026031\n",
      "=== Start Batch 136 ===\n",
      "161.22475504875183\n",
      "=== Start Batch 137 ===\n",
      "161.7065931558609\n",
      "=== Start Batch 138 ===\n",
      "162.7574644088745\n",
      "=== Start Batch 139 ===\n",
      "163.4247401356697\n",
      "=== Start Batch 140 ===\n",
      "164.10409891605377\n",
      "=== Start Batch 141 ===\n",
      "164.62687933444977\n",
      "=== Start Batch 142 ===\n",
      "165.47868657112122\n",
      "Epoch 2/5, Loss: 1.1571936123854631\n",
      "Start Training\n",
      "=== Start Batch 0 ===\n",
      "0.47085461020469666\n",
      "=== Start Batch 1 ===\n",
      "1.7324479520320892\n",
      "=== Start Batch 2 ===\n",
      "2.5009452998638153\n",
      "=== Start Batch 3 ===\n",
      "2.8624577820301056\n",
      "=== Start Batch 4 ===\n",
      "3.7583533823490143\n",
      "=== Start Batch 5 ===\n",
      "4.275997489690781\n",
      "=== Start Batch 6 ===\n",
      "5.341662019491196\n",
      "=== Start Batch 7 ===\n",
      "5.677998900413513\n",
      "=== Start Batch 8 ===\n",
      "6.770570278167725\n",
      "=== Start Batch 9 ===\n",
      "7.792763710021973\n",
      "=== Start Batch 10 ===\n",
      "8.116587460041046\n",
      "=== Start Batch 11 ===\n",
      "8.980814039707184\n",
      "=== Start Batch 12 ===\n",
      "9.656769096851349\n",
      "=== Start Batch 13 ===\n",
      "10.023820132017136\n",
      "=== Start Batch 14 ===\n",
      "10.17437419295311\n",
      "=== Start Batch 15 ===\n",
      "10.870285242795944\n",
      "=== Start Batch 16 ===\n",
      "11.369279593229294\n",
      "=== Start Batch 17 ===\n",
      "12.045098453760147\n",
      "=== Start Batch 18 ===\n",
      "12.541914016008377\n",
      "=== Start Batch 19 ===\n",
      "13.381672531366348\n",
      "=== Start Batch 20 ===\n",
      "13.618669658899307\n",
      "=== Start Batch 21 ===\n",
      "14.989861160516739\n",
      "=== Start Batch 22 ===\n",
      "15.870155900716782\n",
      "=== Start Batch 23 ===\n",
      "16.47902622818947\n",
      "=== Start Batch 24 ===\n",
      "17.03128030896187\n",
      "=== Start Batch 25 ===\n",
      "18.619692474603653\n",
      "=== Start Batch 26 ===\n",
      "19.716743856668472\n",
      "=== Start Batch 27 ===\n",
      "19.911388039588928\n",
      "=== Start Batch 28 ===\n",
      "20.23823657631874\n",
      "=== Start Batch 29 ===\n",
      "21.340524584054947\n",
      "=== Start Batch 30 ===\n",
      "21.93260195851326\n",
      "=== Start Batch 31 ===\n",
      "22.99086931347847\n",
      "=== Start Batch 32 ===\n",
      "23.54339364171028\n",
      "=== Start Batch 33 ===\n",
      "24.254476815462112\n",
      "=== Start Batch 34 ===\n",
      "24.475056633353233\n",
      "=== Start Batch 35 ===\n",
      "25.48249362409115\n",
      "=== Start Batch 36 ===\n",
      "26.152844950556755\n",
      "=== Start Batch 37 ===\n",
      "26.368898957967758\n",
      "=== Start Batch 38 ===\n",
      "26.650202929973602\n",
      "=== Start Batch 39 ===\n",
      "26.96484512090683\n",
      "=== Start Batch 40 ===\n",
      "27.42289423942566\n",
      "=== Start Batch 41 ===\n",
      "27.922769457101822\n",
      "=== Start Batch 42 ===\n",
      "29.50523367524147\n",
      "=== Start Batch 43 ===\n",
      "29.76098856329918\n",
      "=== Start Batch 44 ===\n",
      "30.1697279214859\n",
      "=== Start Batch 45 ===\n",
      "31.619036197662354\n",
      "=== Start Batch 46 ===\n",
      "32.251530170440674\n",
      "=== Start Batch 47 ===\n",
      "32.95722424983978\n",
      "=== Start Batch 48 ===\n",
      "34.02507174015045\n",
      "=== Start Batch 49 ===\n",
      "35.026745080947876\n",
      "=== Start Batch 50 ===\n",
      "35.74309653043747\n",
      "=== Start Batch 51 ===\n",
      "35.93355542421341\n",
      "=== Start Batch 52 ===\n",
      "36.92387706041336\n",
      "=== Start Batch 53 ===\n",
      "37.51690989732742\n",
      "=== Start Batch 54 ===\n",
      "37.95374992489815\n",
      "=== Start Batch 55 ===\n",
      "39.49432197213173\n",
      "=== Start Batch 56 ===\n",
      "39.94809111952782\n",
      "=== Start Batch 57 ===\n",
      "41.150431245565414\n",
      "=== Start Batch 58 ===\n",
      "42.39183089137077\n",
      "=== Start Batch 59 ===\n",
      "43.10965070128441\n",
      "=== Start Batch 60 ===\n",
      "43.748065918684006\n",
      "=== Start Batch 61 ===\n",
      "44.73800012469292\n",
      "=== Start Batch 62 ===\n",
      "46.14566919207573\n",
      "=== Start Batch 63 ===\n",
      "46.740104764699936\n",
      "=== Start Batch 64 ===\n",
      "47.67111572623253\n",
      "=== Start Batch 65 ===\n",
      "48.56877037882805\n",
      "=== Start Batch 66 ===\n",
      "50.27047261595726\n",
      "=== Start Batch 67 ===\n",
      "51.38978371024132\n",
      "=== Start Batch 68 ===\n",
      "51.74512326717377\n",
      "=== Start Batch 69 ===\n",
      "52.430355072021484\n",
      "=== Start Batch 70 ===\n",
      "52.79184332489967\n",
      "=== Start Batch 71 ===\n",
      "54.21582308411598\n",
      "=== Start Batch 72 ===\n",
      "55.354597479104996\n",
      "=== Start Batch 73 ===\n",
      "56.169445246458054\n",
      "=== Start Batch 74 ===\n",
      "56.97107324004173\n",
      "=== Start Batch 75 ===\n",
      "57.91670820116997\n",
      "=== Start Batch 76 ===\n",
      "59.04416581988335\n",
      "=== Start Batch 77 ===\n",
      "59.49821883440018\n",
      "=== Start Batch 78 ===\n",
      "60.51038032770157\n",
      "=== Start Batch 79 ===\n",
      "61.133843779563904\n",
      "=== Start Batch 80 ===\n",
      "61.27912938594818\n",
      "=== Start Batch 81 ===\n",
      "62.11513692140579\n",
      "=== Start Batch 82 ===\n",
      "63.16542738676071\n",
      "=== Start Batch 83 ===\n",
      "63.3734317868948\n",
      "=== Start Batch 84 ===\n",
      "63.78496481478214\n",
      "=== Start Batch 85 ===\n",
      "64.65022529661655\n",
      "=== Start Batch 86 ===\n",
      "65.0061553567648\n",
      "=== Start Batch 87 ===\n",
      "65.96814088523388\n",
      "=== Start Batch 88 ===\n",
      "66.67826120555401\n",
      "=== Start Batch 89 ===\n",
      "67.00306443870068\n",
      "=== Start Batch 90 ===\n",
      "67.93545739352703\n",
      "=== Start Batch 91 ===\n",
      "68.81822298467159\n",
      "=== Start Batch 92 ===\n",
      "69.5811577886343\n",
      "=== Start Batch 93 ===\n",
      "70.23958866298199\n",
      "=== Start Batch 94 ===\n",
      "71.00780610740185\n",
      "=== Start Batch 95 ===\n",
      "72.204361602664\n",
      "=== Start Batch 96 ===\n",
      "72.38795538246632\n",
      "=== Start Batch 97 ===\n",
      "73.66610427200794\n",
      "=== Start Batch 98 ===\n",
      "74.21559669077396\n",
      "=== Start Batch 99 ===\n",
      "74.96850647032261\n",
      "=== Start Batch 100 ===\n",
      "75.75328232347965\n",
      "=== Start Batch 101 ===\n",
      "76.86893691122532\n",
      "=== Start Batch 102 ===\n",
      "77.43772496283054\n",
      "=== Start Batch 103 ===\n",
      "78.39150519669056\n",
      "=== Start Batch 104 ===\n",
      "79.2595107704401\n",
      "=== Start Batch 105 ===\n",
      "79.97946406900883\n",
      "=== Start Batch 106 ===\n",
      "80.61582036316395\n",
      "=== Start Batch 107 ===\n",
      "81.26787365972996\n",
      "=== Start Batch 108 ===\n",
      "81.86677093803883\n",
      "=== Start Batch 109 ===\n",
      "82.31396539509296\n",
      "=== Start Batch 110 ===\n",
      "82.78553099930286\n",
      "=== Start Batch 111 ===\n",
      "83.88468225300312\n",
      "=== Start Batch 112 ===\n",
      "85.2402219325304\n",
      "=== Start Batch 113 ===\n",
      "86.3576502352953\n",
      "=== Start Batch 114 ===\n",
      "87.02453465759754\n",
      "=== Start Batch 115 ===\n",
      "87.82204748690128\n",
      "=== Start Batch 116 ===\n",
      "88.38018895685673\n",
      "=== Start Batch 117 ===\n",
      "88.73367257416248\n",
      "=== Start Batch 118 ===\n",
      "89.18532906472683\n",
      "=== Start Batch 119 ===\n",
      "90.27227817475796\n",
      "=== Start Batch 120 ===\n",
      "91.74137960374355\n",
      "=== Start Batch 121 ===\n",
      "92.27925519645214\n",
      "=== Start Batch 122 ===\n",
      "92.77742798626423\n",
      "=== Start Batch 123 ===\n",
      "93.1631570905447\n",
      "=== Start Batch 124 ===\n",
      "93.74733935296535\n",
      "=== Start Batch 125 ===\n",
      "95.27330385148525\n",
      "=== Start Batch 126 ===\n",
      "95.91128252446651\n",
      "=== Start Batch 127 ===\n",
      "97.40294550359249\n",
      "=== Start Batch 128 ===\n",
      "98.20914350450039\n",
      "=== Start Batch 129 ===\n",
      "99.08553873002529\n",
      "=== Start Batch 130 ===\n",
      "99.54697321355343\n",
      "=== Start Batch 131 ===\n",
      "100.01128096878529\n",
      "=== Start Batch 132 ===\n",
      "101.02595038712025\n",
      "=== Start Batch 133 ===\n",
      "101.71082045137882\n",
      "=== Start Batch 134 ===\n",
      "102.5173284560442\n",
      "=== Start Batch 135 ===\n",
      "103.17661578953266\n",
      "=== Start Batch 136 ===\n",
      "104.1291825324297\n",
      "=== Start Batch 137 ===\n",
      "104.56794364750385\n",
      "=== Start Batch 138 ===\n",
      "105.57219071686268\n",
      "=== Start Batch 139 ===\n",
      "107.16436536610126\n",
      "=== Start Batch 140 ===\n",
      "107.7841741591692\n",
      "=== Start Batch 141 ===\n",
      "108.55289633572102\n",
      "=== Start Batch 142 ===\n",
      "109.42858870327473\n",
      "Epoch 3/5, Loss: 0.7652348860368862\n",
      "Start Training\n",
      "=== Start Batch 0 ===\n",
      "0.30755650997161865\n",
      "=== Start Batch 1 ===\n",
      "0.6605538427829742\n",
      "=== Start Batch 2 ===\n",
      "1.525003045797348\n",
      "=== Start Batch 3 ===\n",
      "1.8864338397979736\n",
      "=== Start Batch 4 ===\n",
      "2.569287896156311\n",
      "=== Start Batch 5 ===\n",
      "3.0198878347873688\n",
      "=== Start Batch 6 ===\n",
      "3.264874219894409\n",
      "=== Start Batch 7 ===\n",
      "3.598047822713852\n",
      "=== Start Batch 8 ===\n",
      "4.0270639061927795\n",
      "=== Start Batch 9 ===\n",
      "4.391501545906067\n",
      "=== Start Batch 10 ===\n",
      "4.949227869510651\n",
      "=== Start Batch 11 ===\n",
      "5.578567147254944\n",
      "=== Start Batch 12 ===\n",
      "6.033764898777008\n",
      "=== Start Batch 13 ===\n",
      "7.044096529483795\n",
      "=== Start Batch 14 ===\n",
      "8.376041114330292\n",
      "=== Start Batch 15 ===\n",
      "8.882538378238678\n",
      "=== Start Batch 16 ===\n",
      "9.042304813861847\n",
      "=== Start Batch 17 ===\n",
      "10.081168830394745\n",
      "=== Start Batch 18 ===\n",
      "10.683635950088501\n",
      "=== Start Batch 19 ===\n",
      "11.230667352676392\n",
      "=== Start Batch 20 ===\n",
      "11.468941286206245\n",
      "=== Start Batch 21 ===\n",
      "12.632828786969185\n",
      "=== Start Batch 22 ===\n",
      "12.895384952425957\n",
      "=== Start Batch 23 ===\n",
      "13.277743503451347\n",
      "=== Start Batch 24 ===\n",
      "14.39266137778759\n",
      "=== Start Batch 25 ===\n",
      "15.189109906554222\n",
      "=== Start Batch 26 ===\n",
      "15.526735171675682\n",
      "=== Start Batch 27 ===\n",
      "15.852854385972023\n",
      "=== Start Batch 28 ===\n",
      "16.713160172104836\n",
      "=== Start Batch 29 ===\n",
      "17.58224691450596\n",
      "=== Start Batch 30 ===\n",
      "18.14061839878559\n",
      "=== Start Batch 31 ===\n",
      "18.603078231215477\n",
      "=== Start Batch 32 ===\n",
      "19.098478719592094\n",
      "=== Start Batch 33 ===\n",
      "19.387569591403008\n",
      "=== Start Batch 34 ===\n",
      "19.87289108335972\n",
      "=== Start Batch 35 ===\n",
      "21.045082941651344\n",
      "=== Start Batch 36 ===\n",
      "21.32987852394581\n",
      "=== Start Batch 37 ===\n",
      "22.834452226758003\n",
      "=== Start Batch 38 ===\n",
      "23.714174166321754\n",
      "=== Start Batch 39 ===\n",
      "24.630448654294014\n",
      "=== Start Batch 40 ===\n",
      "25.744758442044258\n",
      "=== Start Batch 41 ===\n",
      "26.17563809454441\n",
      "=== Start Batch 42 ===\n",
      "26.28022811561823\n",
      "=== Start Batch 43 ===\n",
      "26.519074864685535\n",
      "=== Start Batch 44 ===\n",
      "26.680535845458508\n",
      "=== Start Batch 45 ===\n",
      "27.08911993354559\n",
      "=== Start Batch 46 ===\n",
      "27.143909707665443\n",
      "=== Start Batch 47 ===\n",
      "27.413148269057274\n",
      "=== Start Batch 48 ===\n",
      "27.942371234297752\n",
      "=== Start Batch 49 ===\n",
      "28.448562547564507\n",
      "=== Start Batch 50 ===\n",
      "28.943632528185844\n",
      "=== Start Batch 51 ===\n",
      "29.37878553569317\n",
      "=== Start Batch 52 ===\n",
      "29.91702102124691\n",
      "=== Start Batch 53 ===\n",
      "30.493176743388176\n",
      "=== Start Batch 54 ===\n",
      "30.871661856770515\n",
      "=== Start Batch 55 ===\n",
      "31.462310925126076\n",
      "=== Start Batch 56 ===\n",
      "32.02128280699253\n",
      "=== Start Batch 57 ===\n",
      "32.37515400350094\n",
      "=== Start Batch 58 ===\n",
      "33.13031290471554\n",
      "=== Start Batch 59 ===\n",
      "34.07567010819912\n",
      "=== Start Batch 60 ===\n",
      "34.35778979957104\n",
      "=== Start Batch 61 ===\n",
      "34.88038121163845\n",
      "=== Start Batch 62 ===\n",
      "34.981426656246185\n",
      "=== Start Batch 63 ===\n",
      "35.75683808326721\n",
      "=== Start Batch 64 ===\n",
      "36.31536555290222\n",
      "=== Start Batch 65 ===\n",
      "37.05170238018036\n",
      "=== Start Batch 66 ===\n",
      "38.11039364337921\n",
      "=== Start Batch 67 ===\n",
      "39.340479612350464\n",
      "=== Start Batch 68 ===\n",
      "39.43114875257015\n",
      "=== Start Batch 69 ===\n",
      "39.51766883581877\n",
      "=== Start Batch 70 ===\n",
      "40.02883445471525\n",
      "=== Start Batch 71 ===\n",
      "41.37736450880766\n",
      "=== Start Batch 72 ===\n",
      "41.481909327208996\n",
      "=== Start Batch 73 ===\n",
      "42.073448054492474\n",
      "=== Start Batch 74 ===\n",
      "42.24942146986723\n",
      "=== Start Batch 75 ===\n",
      "42.45503210276365\n",
      "=== Start Batch 76 ===\n",
      "42.713083915412426\n",
      "=== Start Batch 77 ===\n",
      "43.68630253523588\n",
      "=== Start Batch 78 ===\n",
      "44.210759453475475\n",
      "=== Start Batch 79 ===\n",
      "44.98802422732115\n",
      "=== Start Batch 80 ===\n",
      "45.10022062808275\n",
      "=== Start Batch 81 ===\n",
      "46.83947116881609\n",
      "=== Start Batch 82 ===\n",
      "46.99781907349825\n",
      "=== Start Batch 83 ===\n",
      "48.15663016587496\n",
      "=== Start Batch 84 ===\n",
      "48.26799713075161\n",
      "=== Start Batch 85 ===\n",
      "49.068544551730156\n",
      "=== Start Batch 86 ===\n",
      "49.74658899009228\n",
      "=== Start Batch 87 ===\n",
      "50.48958437144756\n",
      "=== Start Batch 88 ===\n",
      "50.90406034886837\n",
      "=== Start Batch 89 ===\n",
      "51.93002913892269\n",
      "=== Start Batch 90 ===\n",
      "52.95080004632473\n",
      "=== Start Batch 91 ===\n",
      "53.44801576435566\n",
      "=== Start Batch 92 ===\n",
      "53.952990368008614\n",
      "=== Start Batch 93 ===\n",
      "55.001977279782295\n",
      "=== Start Batch 94 ===\n",
      "55.47907270491123\n",
      "=== Start Batch 95 ===\n",
      "56.03364385664463\n",
      "=== Start Batch 96 ===\n",
      "56.66608162224293\n",
      "=== Start Batch 97 ===\n",
      "57.297009125351906\n",
      "=== Start Batch 98 ===\n",
      "57.802870228886604\n",
      "=== Start Batch 99 ===\n",
      "57.90855799615383\n",
      "=== Start Batch 100 ===\n",
      "58.07056836783886\n",
      "=== Start Batch 101 ===\n",
      "58.90955053269863\n",
      "=== Start Batch 102 ===\n",
      "59.101990684866905\n",
      "=== Start Batch 103 ===\n",
      "59.567151978611946\n",
      "=== Start Batch 104 ===\n",
      "59.86935140192509\n",
      "=== Start Batch 105 ===\n",
      "59.953273840248585\n",
      "=== Start Batch 106 ===\n",
      "60.34582967311144\n",
      "=== Start Batch 107 ===\n",
      "61.092309065163136\n",
      "=== Start Batch 108 ===\n",
      "61.18314213305712\n",
      "=== Start Batch 109 ===\n",
      "62.25352222472429\n",
      "=== Start Batch 110 ===\n",
      "62.74829898029566\n",
      "=== Start Batch 111 ===\n",
      "63.555211402475834\n",
      "=== Start Batch 112 ===\n",
      "64.35559610277414\n",
      "=== Start Batch 113 ===\n",
      "64.52093473821878\n",
      "=== Start Batch 114 ===\n",
      "65.69590858370066\n",
      "=== Start Batch 115 ===\n",
      "66.30699823051691\n",
      "=== Start Batch 116 ===\n",
      "66.67372114211321\n",
      "=== Start Batch 117 ===\n",
      "67.7755711749196\n",
      "=== Start Batch 118 ===\n",
      "68.88113612681627\n",
      "=== Start Batch 119 ===\n",
      "69.36870405822992\n",
      "=== Start Batch 120 ===\n",
      "69.97471687942743\n",
      "=== Start Batch 121 ===\n",
      "70.58191231638193\n",
      "=== Start Batch 122 ===\n",
      "71.1942807212472\n",
      "=== Start Batch 123 ===\n",
      "71.28937811404467\n",
      "=== Start Batch 124 ===\n",
      "72.31623274832964\n",
      "=== Start Batch 125 ===\n",
      "72.77193666249514\n",
      "=== Start Batch 126 ===\n",
      "74.31485737115145\n",
      "=== Start Batch 127 ===\n",
      "74.39938861131668\n",
      "=== Start Batch 128 ===\n",
      "74.71407732367516\n",
      "=== Start Batch 129 ===\n",
      "75.27873811125755\n",
      "=== Start Batch 130 ===\n",
      "75.34135811030865\n",
      "=== Start Batch 131 ===\n",
      "76.07181857526302\n",
      "=== Start Batch 132 ===\n",
      "76.2601567208767\n",
      "=== Start Batch 133 ===\n",
      "76.83208230137825\n",
      "=== Start Batch 134 ===\n",
      "78.08886578679085\n",
      "=== Start Batch 135 ===\n",
      "78.83450230956078\n",
      "=== Start Batch 136 ===\n",
      "79.5453527867794\n",
      "=== Start Batch 137 ===\n",
      "80.53908267617226\n",
      "=== Start Batch 138 ===\n",
      "80.96165192127228\n",
      "=== Start Batch 139 ===\n",
      "81.78772503137589\n",
      "=== Start Batch 140 ===\n",
      "82.65022486448288\n",
      "=== Start Batch 141 ===\n",
      "83.44008356332779\n",
      "=== Start Batch 142 ===\n",
      "83.54305017739534\n",
      "Epoch 4/5, Loss: 0.5842171341076597\n",
      "Start Training\n",
      "=== Start Batch 0 ===\n",
      "0.22021104395389557\n",
      "=== Start Batch 1 ===\n",
      "0.4280148446559906\n",
      "=== Start Batch 2 ===\n",
      "1.3581739366054535\n",
      "=== Start Batch 3 ===\n",
      "1.6244336068630219\n",
      "=== Start Batch 4 ===\n",
      "1.8575942069292068\n",
      "=== Start Batch 5 ===\n",
      "1.9981954991817474\n",
      "=== Start Batch 6 ===\n",
      "2.3204084038734436\n",
      "=== Start Batch 7 ===\n",
      "3.1529536843299866\n",
      "=== Start Batch 8 ===\n",
      "3.566140443086624\n",
      "=== Start Batch 9 ===\n",
      "3.6264224871993065\n",
      "=== Start Batch 10 ===\n",
      "4.061870448291302\n",
      "=== Start Batch 11 ===\n",
      "4.282250337302685\n",
      "=== Start Batch 12 ===\n",
      "5.073819808661938\n",
      "=== Start Batch 13 ===\n",
      "5.543699912726879\n",
      "=== Start Batch 14 ===\n",
      "7.010211043059826\n",
      "=== Start Batch 15 ===\n",
      "7.203930981457233\n",
      "=== Start Batch 16 ===\n",
      "7.740165658295155\n",
      "=== Start Batch 17 ===\n",
      "8.244299538433552\n",
      "=== Start Batch 18 ===\n",
      "8.6027265265584\n",
      "=== Start Batch 19 ===\n",
      "8.700300015509129\n",
      "=== Start Batch 20 ===\n",
      "9.363547123968601\n",
      "=== Start Batch 21 ===\n",
      "9.644798286259174\n",
      "=== Start Batch 22 ===\n",
      "9.718881577253342\n",
      "=== Start Batch 23 ===\n",
      "9.917257621884346\n",
      "=== Start Batch 24 ===\n",
      "10.186919435858727\n",
      "=== Start Batch 25 ===\n",
      "11.287700161337852\n",
      "=== Start Batch 26 ===\n",
      "11.900737687945366\n",
      "=== Start Batch 27 ===\n",
      "12.047639772295952\n",
      "=== Start Batch 28 ===\n",
      "12.391249850392342\n",
      "=== Start Batch 29 ===\n",
      "12.743278995156288\n",
      "=== Start Batch 30 ===\n",
      "12.986248582601547\n",
      "=== Start Batch 31 ===\n",
      "13.275079160928726\n",
      "=== Start Batch 32 ===\n",
      "14.335561782121658\n",
      "=== Start Batch 33 ===\n",
      "15.797010570764542\n",
      "=== Start Batch 34 ===\n",
      "16.166932463645935\n",
      "=== Start Batch 35 ===\n",
      "16.208462953567505\n",
      "=== Start Batch 36 ===\n",
      "16.371618777513504\n",
      "=== Start Batch 37 ===\n",
      "16.73051008582115\n",
      "=== Start Batch 38 ===\n",
      "17.67516902089119\n",
      "=== Start Batch 39 ===\n",
      "17.96840626001358\n",
      "=== Start Batch 40 ===\n",
      "18.17376407980919\n",
      "=== Start Batch 41 ===\n",
      "18.393078863620758\n",
      "=== Start Batch 42 ===\n",
      "18.47135552763939\n",
      "=== Start Batch 43 ===\n",
      "19.29743704199791\n",
      "=== Start Batch 44 ===\n",
      "20.373445719480515\n",
      "=== Start Batch 45 ===\n",
      "21.02179554104805\n",
      "=== Start Batch 46 ===\n",
      "21.433177709579468\n",
      "=== Start Batch 47 ===\n",
      "21.91196921467781\n",
      "=== Start Batch 48 ===\n",
      "22.238378793001175\n",
      "=== Start Batch 49 ===\n",
      "22.912878185510635\n",
      "=== Start Batch 50 ===\n",
      "23.29582390189171\n",
      "=== Start Batch 51 ===\n",
      "23.7189984023571\n",
      "=== Start Batch 52 ===\n",
      "24.155592769384384\n",
      "=== Start Batch 53 ===\n",
      "24.79944536089897\n",
      "=== Start Batch 54 ===\n",
      "25.264446049928665\n",
      "=== Start Batch 55 ===\n",
      "25.408377453684807\n",
      "=== Start Batch 56 ===\n",
      "26.454987570643425\n",
      "=== Start Batch 57 ===\n",
      "27.164450511336327\n",
      "=== Start Batch 58 ===\n",
      "27.4860747307539\n",
      "=== Start Batch 59 ===\n",
      "27.749078020453453\n",
      "=== Start Batch 60 ===\n",
      "28.039692863821983\n",
      "=== Start Batch 61 ===\n",
      "28.904343709349632\n",
      "=== Start Batch 62 ===\n",
      "29.097649425268173\n",
      "=== Start Batch 63 ===\n",
      "29.319012567400932\n",
      "=== Start Batch 64 ===\n",
      "29.458061680197716\n",
      "=== Start Batch 65 ===\n",
      "30.062345191836357\n",
      "=== Start Batch 66 ===\n",
      "30.64518217742443\n",
      "=== Start Batch 67 ===\n",
      "31.645772263407707\n",
      "=== Start Batch 68 ===\n",
      "32.812467738986015\n",
      "=== Start Batch 69 ===\n",
      "33.98998253047466\n",
      "=== Start Batch 70 ===\n",
      "34.459564462304115\n",
      "=== Start Batch 71 ===\n",
      "35.61715020239353\n",
      "=== Start Batch 72 ===\n",
      "35.840939447283745\n",
      "=== Start Batch 73 ===\n",
      "36.66679076850414\n",
      "=== Start Batch 74 ===\n",
      "37.52139614522457\n",
      "=== Start Batch 75 ===\n",
      "38.30775837600231\n",
      "=== Start Batch 76 ===\n",
      "38.43662004172802\n",
      "=== Start Batch 77 ===\n",
      "39.06361673772335\n",
      "=== Start Batch 78 ===\n",
      "39.32589994370937\n",
      "=== Start Batch 79 ===\n",
      "39.560434356331825\n",
      "=== Start Batch 80 ===\n",
      "40.48312450945377\n",
      "=== Start Batch 81 ===\n",
      "40.70578154921532\n",
      "=== Start Batch 82 ===\n",
      "41.55579832196236\n",
      "=== Start Batch 83 ===\n",
      "42.49524423480034\n",
      "=== Start Batch 84 ===\n",
      "43.648719400167465\n",
      "=== Start Batch 85 ===\n",
      "44.375354796648026\n",
      "=== Start Batch 86 ===\n",
      "45.83610597252846\n",
      "=== Start Batch 87 ===\n",
      "46.3194654583931\n",
      "=== Start Batch 88 ===\n",
      "46.48464626073837\n",
      "=== Start Batch 89 ===\n",
      "46.5440410040319\n",
      "=== Start Batch 90 ===\n",
      "47.06260457262397\n",
      "=== Start Batch 91 ===\n",
      "47.29071303829551\n",
      "=== Start Batch 92 ===\n",
      "47.78730347380042\n",
      "=== Start Batch 93 ===\n",
      "48.386970188468695\n",
      "=== Start Batch 94 ===\n",
      "48.98863687738776\n",
      "=== Start Batch 95 ===\n",
      "49.20436682924628\n",
      "=== Start Batch 96 ===\n",
      "49.38287044689059\n",
      "=== Start Batch 97 ===\n",
      "51.04185689613223\n",
      "=== Start Batch 98 ===\n",
      "52.662235867232084\n",
      "=== Start Batch 99 ===\n",
      "53.997244250029325\n",
      "=== Start Batch 100 ===\n",
      "54.44479928538203\n",
      "=== Start Batch 101 ===\n",
      "54.75020266696811\n",
      "=== Start Batch 102 ===\n",
      "55.11013485118747\n",
      "=== Start Batch 103 ===\n",
      "55.483728747814894\n",
      "=== Start Batch 104 ===\n",
      "55.67163832113147\n",
      "=== Start Batch 105 ===\n",
      "55.7983496747911\n",
      "=== Start Batch 106 ===\n",
      "56.301688369363546\n",
      "=== Start Batch 107 ===\n",
      "57.50279849395156\n",
      "=== Start Batch 108 ===\n",
      "57.60907348617911\n",
      "=== Start Batch 109 ===\n",
      "57.68166792020202\n",
      "=== Start Batch 110 ===\n",
      "58.48866289481521\n",
      "=== Start Batch 111 ===\n",
      "58.94043650850654\n",
      "=== Start Batch 112 ===\n",
      "59.16875817999244\n",
      "=== Start Batch 113 ===\n",
      "59.41552461311221\n",
      "=== Start Batch 114 ===\n",
      "61.01796902343631\n",
      "=== Start Batch 115 ===\n",
      "61.230312030762434\n",
      "=== Start Batch 116 ===\n",
      "61.52723593637347\n",
      "=== Start Batch 117 ===\n",
      "62.53383107110858\n",
      "=== Start Batch 118 ===\n",
      "62.74825058504939\n",
      "=== Start Batch 119 ===\n",
      "63.85009799525142\n",
      "=== Start Batch 120 ===\n",
      "64.82490788027644\n",
      "=== Start Batch 121 ===\n",
      "65.90095971152186\n",
      "=== Start Batch 122 ===\n",
      "66.24842168018222\n",
      "=== Start Batch 123 ===\n",
      "66.6579758040607\n",
      "=== Start Batch 124 ===\n",
      "67.02582360431552\n",
      "=== Start Batch 125 ===\n",
      "67.91598696634173\n",
      "=== Start Batch 126 ===\n",
      "67.96981991827488\n",
      "=== Start Batch 127 ===\n",
      "68.14950804412365\n",
      "=== Start Batch 128 ===\n",
      "68.65358905494213\n",
      "=== Start Batch 129 ===\n",
      "68.829843506217\n",
      "=== Start Batch 130 ===\n",
      "69.0104192495346\n",
      "=== Start Batch 131 ===\n",
      "69.66655904054642\n",
      "=== Start Batch 132 ===\n",
      "69.83061157166958\n",
      "=== Start Batch 133 ===\n",
      "69.88973827287555\n",
      "=== Start Batch 134 ===\n",
      "70.0372513346374\n",
      "=== Start Batch 135 ===\n",
      "70.08987851440907\n",
      "=== Start Batch 136 ===\n",
      "70.40140791237354\n",
      "=== Start Batch 137 ===\n",
      "70.60853061079979\n",
      "=== Start Batch 138 ===\n",
      "70.93968117237091\n",
      "=== Start Batch 139 ===\n",
      "71.64682948589325\n",
      "=== Start Batch 140 ===\n",
      "71.8651793897152\n",
      "=== Start Batch 141 ===\n",
      "71.93212071806192\n",
      "=== Start Batch 142 ===\n",
      "72.17623759061098\n",
      "Epoch 5/5, Loss: 0.5047289342000768\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 5  # You can adjust this based on your requirements\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model and data to the device\n",
    "model.to(device)\n",
    "input_ids_train, attention_masks_train, labels_train = input_ids_train.to(device), attention_masks_train.to(device), labels_train.to(device)\n",
    "loss_per_epoch = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(\"Start Training\")\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for batch in dataloader_train:\n",
    "        print(\"=== Start Batch \" + str(n) + \" ===\")\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = (\n",
    "            batch_input_ids.to(device),\n",
    "            batch_attention_masks.to(device),\n",
    "            batch_labels.to(device),\n",
    "        )\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch_input_ids,\n",
    "            attention_mask=batch_attention_masks,\n",
    "            labels=batch_labels,\n",
    "        )\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(total_loss)\n",
    "\n",
    "        n = n+1\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataloader_train)\n",
    "    loss_per_epoch.append(average_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
    "\n",
    "# Save the trained model (This method is specific to the transformers library and is designed for saving transformer-based models.\n",
    "# It saves the model in a format that includes the architecture, parameters, and additional information specific to the transformers library.\n",
    "# It provides a higher-level abstraction that is specific to transformer models and allows for easily loading the model using AutoModel.from_pretrained later.)\n",
    "model.save_pretrained(\"/Users/Hsuweic/Desktop/AI4health/model/BERTweet model\")\n",
    "tokenizer.save_pretrained(\"/Users/Hsuweic/Desktop/AI4health/model/BERTweet model\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss values\n",
    "plt.plot(range(1, len(loss_per_epoch) + 1), loss_per_epoch, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model\n",
    "# batch size = 8\n",
    "# epoch = 5\n",
    "# total training time = 190 mins\n",
    "# optimizer = AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       109\n",
      "           1       0.91      0.95      0.93        78\n",
      "           2       0.68      0.76      0.72        50\n",
      "           3       0.89      0.62      0.73        13\n",
      "           4       0.50      0.38      0.43         8\n",
      "           5       0.89      0.50      0.64        16\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         6\n",
      "           8       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.80       286\n",
      "   macro avg       0.52      0.46      0.48       286\n",
      "weighted avg       0.77      0.80      0.78       286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input data for testing set\n",
    "tokenized_test = X_test.apply(tokenize_data)\n",
    "# print(len(tokenized_test))\n",
    "# print(tokenized_test)\n",
    "\n",
    "input_ids_test = torch.cat([tokenized_test[i]['input_ids'] for i in range(len(tokenized_test))], dim=0)\n",
    "attention_masks_test = torch.cat([tokenized_test[i]['attention_mask'] for i in range(len(tokenized_test))], dim=0)\n",
    "labels_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Create DataLoader for testing set\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=False)\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_test:\n",
    "        input_ids, attention_masks, labels = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "Label Dictionary:\n",
    "{'Race': 0, 'Sexual Orientation': 1, 'Gender': 2, 'Religion': 3, 'Disability': 4, 'Physical Appearance': 5, 'Class': 6, 'Ethnicity': 7, 'Behavior': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Example input tweet\n",
    "input_tweet = \"you are a nasty bitch and I hate you\"\n",
    "\n",
    "# Load the BERTweet tokenizer and model\n",
    "model_path = \"/Users/Hsuweic/Desktop/AI4health/model/BERTweet classification model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=9)  # Assuming 9 classes\n",
    "\n",
    "# Tokenize the input tweet\n",
    "inputs = tokenizer(input_tweet, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through the model to obtain logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get logits from the output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# Choose the predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
