{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load BERTweet model and tokenizer\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "num_categories = 9  # Number of classes/categories\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1430, 5)\n",
      "Index(['Unnamed: 0', 'tweet', 'label', 'categories', 'cleaned_tweet'], dtype='object')\n",
      "1430\n"
     ]
    }
   ],
   "source": [
    "import preprocessor\n",
    "import demoji\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def preprocess_text(text):\n",
    "    # Use tweet-preprocessor to clean tweets\n",
    "    cleaned_text = preprocessor.clean(text)\n",
    "    # Remove emojis\n",
    "    cleaned_text = remove_emojis(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "# Load your dataset with columns 'tweet' and 'categories'\n",
    "df = pd.read_csv(\"/Users/Hsuweic/Desktop/AI4healthcare/dataset_categories.csv\")\n",
    "hate_speech = df[df['label'] == 1].copy() \n",
    "# Reset the index to ensure it is consecutive\n",
    "hate_speech.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preprocess text\n",
    "hate_speech['cleaned_tweet'] = hate_speech['tweet'].apply(preprocess_text)\n",
    "\n",
    "print(hate_speech.shape)\n",
    "print(hate_speech.columns)\n",
    "print(len(hate_speech))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "[1 2 3 4 5 6 7 8 9]\n",
      "1430\n",
      "0       [input_ids, token_type_ids, attention_mask]\n",
      "1       [input_ids, token_type_ids, attention_mask]\n",
      "2       [input_ids, token_type_ids, attention_mask]\n",
      "3       [input_ids, token_type_ids, attention_mask]\n",
      "4       [input_ids, token_type_ids, attention_mask]\n",
      "                           ...                     \n",
      "1425    [input_ids, token_type_ids, attention_mask]\n",
      "1426    [input_ids, token_type_ids, attention_mask]\n",
      "1427    [input_ids, token_type_ids, attention_mask]\n",
      "1428    [input_ids, token_type_ids, attention_mask]\n",
      "1429    [input_ids, token_type_ids, attention_mask]\n",
      "Name: cleaned_tweet, Length: 1430, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input data\n",
    "max_length = 128  # You can adjust this based on your requirements\n",
    "\n",
    "def tokenize_data(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize input data\n",
    "tokenized_data = hate_speech['cleaned_tweet'].apply(tokenize_data)\n",
    "\n",
    "# Convert categories to numerical labels\n",
    "label_dict = {category: idx for idx, category in enumerate(hate_speech['categories'].unique())}\n",
    "hate_speech['label'] = hate_speech['categories'].map(label_dict)\n",
    "\n",
    "print(type(tokenized_data))\n",
    "print(hate_speech['label'].unique())\n",
    "print(len(tokenized_data))\n",
    "print(tokenized_data)\n",
    "\n",
    "# Prepare input tensors\n",
    "input_ids = torch.cat([tokenized_data[i]['input_ids'] for i in range(len(tokenized_data))], dim=0)\n",
    "attention_masks = torch.cat([tokenized_data[i]['attention_mask'] for i in range(len(tokenized_data))], dim=0)\n",
    "labels = torch.tensor(hate_speech['label'].values)\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hsuweic/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 9 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     69\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     71\u001b[0m     input_ids\u001b[39m=\u001b[39;49mbatch_input_ids,\n\u001b[1;32m     72\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mbatch_attention_masks,\n\u001b[1;32m     73\u001b[0m     labels\u001b[39m=\u001b[39;49mbatch_labels,\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m     77\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/transformers/models/roberta/modeling_roberta.py:1232\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1231\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1232\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1233\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1234\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 9 is out of bounds."
     ]
    }
   ],
   "source": [
    "# from transformers import AdamW\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Set optimizer and loss function\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 3  # You can adjust this based on your requirements\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "#         input_ids, attention_mask, label = batch\n",
    "#         input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     average_loss = total_loss / len(dataloader)\n",
    "#     print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save_pretrained(\"bertweet_multi_classification_model\")\n",
    "# tokenizer.save_pretrained(\"bertweet_multi_classification_model\")\n",
    "\n",
    "\n",
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 3  # You can adjust this based on your requirements\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model and data to the device\n",
    "model.to(device)\n",
    "input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = (\n",
    "            batch_input_ids.to(device),\n",
    "            batch_attention_masks.to(device),\n",
    "            batch_labels.to(device),\n",
    "        )\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch_input_ids,\n",
    "            attention_mask=batch_attention_masks,\n",
    "            labels=batch_labels,\n",
    "        )\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"/Users/Hsuweic/Desktop/AI4healthcare/BERTweet classification model\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
